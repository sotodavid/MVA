{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huShIAOf2CCA"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "**Question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyqo3G_w16T8"
   },
   "source": [
    "We want to minimize (over w) the empirical risk. In order to do so, we will apply the stochastic gradient descent algorithm. The latter will look like this :\n",
    "\n",
    "* We choose a $w_0$ randomly and a nonnegative sequence $(\\epsilon_k)_k$ which satisfies some properties (we will detail later).\n",
    "\n",
    "* for k : 1 $\\rightarrow$ end :\n",
    "  * $z_{k+1} \\sim \\mu$ (here $\\mu$ is the uniform distribution on the set of observations ${ \\left\\{z_i = (x_i​,y_i​) \\right\\} }_{1⩽i⩽n}$ )\n",
    "\n",
    "  * Calculate the gradient $\\nabla_w J(w, z)$\n",
    "\n",
    "  * $w_{k+1} = w_k - \\epsilon_k \\nabla_w J(w, z) $\n",
    "\n",
    "* end.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Therefore, we need to compute the gradient of J with respect to w for the stochastic gradient descent. This gradient is given by : $$ \\nabla_w J(w, z) = - 2 x \\left( y - w^\\mathsf{T} x \\right) $$ \n",
    "\n",
    "We need to make sure that all the hypothesis of convergence hold. According to the hypothesis of convergence (H3) seen in class, the gradient $ \\nabla_w J(w, z) $ must be bounded. It is patent that this condition is not satisfied here. Indeed if we take $ w = tx $ and let $t \\to \\infty$ the gradient is clearly not bounded. One solution to this problem is to normalize $w$. This yields : $$ \\left\\| \\nabla_w J(w, z) \\right\\| \\leqslant 2 \\left( 1 + \\| x \\| \\right) \\, \\| x \\| $$\n",
    "Since the number of observation is finite, the norm of $x$ is bounded, and therefore the gradient is bounded. This normalization of w does not affect the hyperplane because the norm of $w$ has no influence on the hyperplane. We will add this step (normalization of w) to our algorithm. $J$ satisfies the rest of the hypothesis of convergence.\n",
    "The last step is to choose a sequence $(\\epsilon_k)_k$ that satisfies the last hypothesis of convergence (H5) of the course, which is : \n",
    "$$ \\sum{\\epsilon_k}  = + \\infty, \\qquad \\sum_{k > 0} \\epsilon_k^2 < + \\infty \\qquad and \\qquad  \\forall k \\qquad \\epsilon_k > 0 \\qquad $$\n",
    "\n",
    "In our algorithm we will choose the sequence of inversed integers as the sequnce of $\\epsilon_k$. The following algorithm converges toward an optimal solution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5neV8HQn18OH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(Z, w0, steps) : \n",
    "\n",
    "\t# This function computes and returns a hyperplane w that separates labeled observations contained in Z using a\n",
    "\t# stochastic gradient descent.\n",
    "\n",
    "\t# Z is the list of observations\n",
    "\t# w0 represents the hyperplane used to start the algorithm\n",
    "\t# epsilon\trepresents the sequence of hypothesis (H5)of the course. Epsilon is used to scale the updates of w\n",
    "\n",
    "\tw = w0\n",
    "\tfor k in range(steps) : \n",
    "\t\t# draw an observation\n",
    "\t\tx, y = Z[np.random.randint(len(Z))]\n",
    "\t\t# compute the gradient\n",
    "\t\tgrad = - 2 * (y - np.dot(w, x)) * x\n",
    "\t\t# update w\n",
    "\t\tepsilon = 1/(k+1)\n",
    "\t\tw = w - epsilon * grad\n",
    "\t\t# normalization of w \n",
    "\t\tw = w/(np.linalg.norm(w))\n",
    "\treturn w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwRNaxpY2Oqa"
   },
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiKpW-1ps1Dp"
   },
   "outputs": [],
   "source": [
    "############ Question 2 #############\n",
    "\n",
    "# For this question we will create a function generate_sample() that samples a set of observations as asked in the question.\n",
    "\n",
    "def drawVector():\n",
    "\t# Return a random 2D vector drawn uniformly on the unit sphere\n",
    "\ttheta = np.random.uniform(0, 2*np.math.pi)\n",
    "\treturn np.array((np.math.cos(theta), np.math.sin(theta)))\n",
    " \n",
    "\n",
    "def generate_sample(n):\n",
    "\t# Function that generates and returns a list of observations z = (x, y).\n",
    "\t# n is the number of observations generated\n",
    "\n",
    "\tw = drawVector()\n",
    "\tprint(\"Real vector w_bar =\", w) # will display the real vector w\n",
    "\tsample = []\n",
    "\tfor i in range(n):\n",
    "\t\tr = np.random.rand()\n",
    "\t\tx = (r ** 0.5) * drawVector() #  x a 2D vector drawn uniformly using the function drawVector()\n",
    "    \n",
    "\t\t# Now we compute the label y of x with respect to the side of the hyperplane w the point x is.\n",
    "\t\t# y takes values in {-1,1}\n",
    "\t\tif np.dot(w, x) > 0 :      # Recall that w is a hyperplane drawn uniformly on the unit sphere\n",
    "\t\t\ty = 1\n",
    "\t\telse :\n",
    "\t\t\ty = -1 \n",
    "\t\tsample.append([x, y])\n",
    "\t\n",
    "\treturn sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4SD5J0ks1Ms",
    "outputId": "51c750bb-7bf5-4b34-a4a2-ef08dd68b66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vector w_bar = [0.23374048 0.97229902]\n",
      " \n",
      "Here is a set of observations (of size n=10) that our function generated :\n",
      " \n",
      "[[array([0.2054331 , 0.03086069]), 1], [array([-0.26944663, -0.37525375]), -1], [array([ 0.78858238, -0.05321048]), 1], [array([-0.62028342,  0.17433375]), 1], [array([-0.43766308,  0.69651809]), 1], [array([-0.37308494, -0.75491072]), -1], [array([-0.75696117, -0.41548621]), -1], [array([ 0.27485382, -0.76610157]), -1], [array([-0.5758344,  0.5532276]), 1], [array([-0.18266714, -0.87203057]), -1]]\n"
     ]
    }
   ],
   "source": [
    "n = 10 # number of observations\n",
    "\n",
    "Z = generate_sample(n)\n",
    "print(\" \")\n",
    "print(\"Here is a set of observations (of size n=10) that our function generated :\")\n",
    "print(\" \")\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMYWjzdE2T4U"
   },
   "source": [
    "**Question 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnXBCxVItMCL"
   },
   "outputs": [],
   "source": [
    "############ Question 3 #############\n",
    "\n",
    "# Before testing our algorithm, we define a function score() that will compute the score of well predicted observations for a given \n",
    "# hyperplane w\n",
    "\n",
    "def score(Z, w):\n",
    "\n",
    "\t# Z is the list of observations\n",
    "\t# w is an array of float and represents the normal vector of the hyperplane for which we want to know the score\n",
    "\n",
    "\tn = len(Z)\n",
    "\tcorrect = 0\n",
    "\tfor x, y in Z:\n",
    "\t\tif y*np.dot(w, x) > 0: \n",
    "\t\t\tcorrect += 1\n",
    "\tscore = correct/n # The score is the ratio between the number of correctly predicted observations and the total number of observations.\n",
    "\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pntwC8lhtmtA",
    "outputId": "6e1bed5e-ea57-4af4-d9bc-53f99a1dac48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vector w_bar = [0.61066322 0.79189042]\n",
      " \n",
      "Vector w estimated : w_* = [0.59954426 0.8003416 ]\n",
      " \n",
      "The score of well predicted observations is score = 1.0\n"
     ]
    }
   ],
   "source": [
    "#### Now we can test our algorithm.\n",
    "\n",
    "# We start by setting the parameters n and steps :\n",
    "\n",
    "n = 100             # Number of observations\n",
    "steps = 500         # Number of steps\n",
    "\n",
    "Z = generate_sample(n) # we generate a sample of observations Z of size n = 100\n",
    "\n",
    "##### Now we test our algortihm stochastic_gradient_descent on the observations Z\n",
    "\n",
    "# we start by generating the vector w0 \n",
    "w0 = drawVector()\n",
    "\n",
    "w_optimal = stochastic_gradient_descent(Z, w0, steps) \n",
    "\n",
    "print(\" \")\n",
    "print(\"Vector w estimated : w_* =\", w_optimal)\n",
    "print(\" \")\n",
    "print(\"The score of well predicted observations is score =\", score(Z, w_optimal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8qPTJBCwbC4"
   },
   "source": [
    "As we can see, the vector w_bar with which the data was generated and the vector w_* estimated by the stochastic gradient descent are almost the same. Moreover, the score of well predicted observations is very high. This example illustrates how effective the stochastic gradient descent can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0COPRBf2YAT"
   },
   "source": [
    "**Question 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT3jdkn-tnBe"
   },
   "outputs": [],
   "source": [
    "############ Question 4 #############\n",
    "\n",
    "# We start by creating a function that noises the observations\n",
    "def addNoise(Z, sigma):\n",
    "\n",
    "\t# Z is the list of observations\n",
    "\t# sigma is the standard deviation used in the gaussian\n",
    "\t\n",
    "\tfor x, y in Z:\n",
    "\t\tx = x + np.random.normal(np.array([0, 0]), sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7pvnGMnwy3K",
    "outputId": "b1cf579e-629e-4b06-caac-2082b2cee0a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vector w_bar = [-0.93149074  0.36376503]\n",
      " \n",
      "Vector w estimated : w_* = [-0.88595575  0.46376978]\n",
      " \n",
      "The score of well predicted observations is score = 0.99\n"
     ]
    }
   ],
   "source": [
    "### Now we can perform the optimization again.\n",
    "\n",
    "# We start by setting the parameters n, sigme and K :\n",
    "n = 100             # Number of observations\n",
    "sigma = 0.2         # Standard deviation of the noise\n",
    "steps = 500             # Number of steps\n",
    "\n",
    "\n",
    "# We generate our sample of observations\n",
    "Z = generate_sample(n)\n",
    "# We add gaussian noise to Z\n",
    "addNoise(Z, sigma) \n",
    "\n",
    "# we generate w0\n",
    "w0 = drawVector()\n",
    "\n",
    "w_optimal = stochastic_gradient_descent(Z, w0, steps) \n",
    "\n",
    "print(\" \")\n",
    "print(\"Vector w estimated : w_* =\", w_optimal)\n",
    "print(\" \")\n",
    "print(\"The score of well predicted observations is score =\", score(Z, w_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ2plRuJx3RN"
   },
   "source": [
    "As we can see, the result here is nearly as good as the result in question 3. The w_* estimated is really close to the real w_bar and the score of well predicted observations is really high, as in question 3. \n",
    "The stochastic gradient descent algorithm seems to handle pretty well the presence of gaussian noise in the observations. Here we chose sigma = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GJQ7D4t2bbc"
   },
   "source": [
    "**Question 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKrYgIj1yvwt",
    "outputId": "ee2c5dbb-b7f9-4811-bec1-fa6240ea846e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "############ Question 5 #############\n",
    "\n",
    "# First of all, we need to upload the new data Breast_Cancer_data.csv and create our sample of\n",
    "# observation. This process takes some lines of code.\n",
    "# The corrector may skip this part and go the next part where we test our algorithm.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/content/Breast_Cancer_data.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "print(df.columns)\n",
    "\n",
    "# We first create the vector y of label : we will denote 0 = M and 1 = B\n",
    "y = []\n",
    "for i in df[\"diagnosis\"] :\n",
    "  if (i == \"M\") : \n",
    "    A = 0\n",
    "    y.append(A)\n",
    "  else :\n",
    "    A = 1\n",
    "    y.append(A)\n",
    "\n",
    "\n",
    "df = df.dropna(how='all', axis=1) #there is an empty column in the data that we need to drop\n",
    "\n",
    "# We create the vector x of features\n",
    "x = []\n",
    "for j in range(len(df[\"id\"])) :\n",
    "  B =[]\n",
    "  for i in df.columns :\n",
    "    if (i != \"diagnosis\") :\n",
    "      B.append(df[i][j])\n",
    "  x.append(B)\n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "Z=[] \n",
    "for i in range(569) :\n",
    "  Z.append([x[i],y[i]])\n",
    "\n",
    "# Z is the set of observations of the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hf_qGPFMy2_k",
    "outputId": "88152d10-727a-40b5-87f7-2c687c383e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Vector w estimated : w_* = [9.99999868e-01 1.06954001e-05 1.87497957e-05 6.86304874e-05\n",
      " 3.12311027e-04 9.44281576e-08 1.02745318e-07 9.37712467e-08\n",
      " 2.55193182e-08 2.26689929e-07 7.93303411e-08 2.05312490e-07\n",
      " 1.04983268e-06 1.59106046e-06 1.34388381e-05 6.62923300e-09\n",
      " 3.86464020e-08 5.59821692e-08 9.47510460e-09 1.94846452e-08\n",
      " 4.48814884e-09 1.19691392e-05 2.56863294e-05 7.93303411e-05\n",
      " 3.93701174e-04 1.37283243e-07 3.80340274e-07 4.83330542e-07\n",
      " 9.04087537e-08 3.32018354e-07 1.09392365e-07]\n",
      " \n",
      "The score of well predicted observations is score = 0.6274165202108963\n"
     ]
    }
   ],
   "source": [
    "############## Testing our algorithm on Breast_Cancer_data.csv ##############\n",
    "\n",
    "# In order to test the algorithm stochastic_gradient_descent on the new data, we need to create a new function drawVector31()\n",
    "# (because the dimension of X here is 569 x 31)\n",
    "\n",
    "def drawVector31():\n",
    "  theta = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta2 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta3 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta4 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta5 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta6 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta7 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta8 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta9 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta10 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta11 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta12 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta13 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta14 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta15 = np.random.uniform(0, 2*np.math.pi)\n",
    "  theta16 = np.random.uniform(0, 2*np.math.pi)\n",
    "  return np.array((np.math.cos(theta), np.math.sin(theta), np.math.cos(theta2), np.math.sin(theta2), np.math.cos(theta3), np.math.sin(theta3),np.math.cos(theta4), np.math.sin(theta4),np.math.cos(theta5), np.math.sin(theta5),np.math.cos(theta6), np.math.sin(theta6),np.math.cos(theta7), np.math.sin(theta7),np.math.cos(theta8), np.math.sin(theta8),np.math.cos(theta9), np.math.sin(theta9),np.math.cos(theta10), np.math.sin(theta10),np.math.cos(theta11), np.math.sin(theta11),np.math.cos(theta12), np.math.sin(theta12),np.math.cos(theta13), np.math.sin(theta13),np.math.cos(theta14), np.math.sin(theta14),np.math.cos(theta15), np.math.sin(theta15),np.math.cos(theta16)))\n",
    "\n",
    "##### We set the parameters for our algorithm\n",
    "\n",
    "steps = 500 # number steps\n",
    "\n",
    "w0 = drawVector31()\n",
    "\n",
    "w_optimal = stochastic_gradient_descent(Z, w0, steps)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Vector w estimated : w_* =\", w_optimal)\n",
    "print(\" \")\n",
    "print(\"The score of well predicted observations is score =\", score(Z, w_optimal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79L_eH6I1bWP"
   },
   "source": [
    "The score of well predicted observations is 0.63.\n",
    "I was expecting a higher score. I don't know if it is normal or if something went wrong in my algorithm. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP1_CompStats_David_Soto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
